{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough: Simulating Greenland glaciers with SERMeQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome!  This notebook will walk you through the necessary steps to simulate one or more Greenland outlet glaciers using SERMeQ.  We will use geographic data and identifiers from MEaSUREs, bed topography from BedMachine v3, and surface mass balance from HIRHAM/ERA-Interim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyproj as pyproj\n",
    "import csv\n",
    "import collections\n",
    "import datetime\n",
    "from matplotlib import cm\n",
    "from scipy import interpolate\n",
    "from scipy.ndimage import gaussian_filter\n",
    "## Special import for SERMeQ modules\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/lizz/Documents/GitHub/plastic-networks')\n",
    "from SERMeQ.plastic_utilities_v2 import *\n",
    "from SERMeQ.GL_model_tools import *\n",
    "from SERMeQ.flowline_class_hierarchy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reading in surface topography')\n",
    "gl_bed_path ='/Users/lizz/Documents/GitHub/Data_unsynced/BedMachine-Greenland/BedMachineGreenland-2017-09-20.nc'\n",
    "fh = Dataset(gl_bed_path, mode='r')\n",
    "xx = fh.variables['x'][:].copy() #x-coord (polar stereo (70, 45))\n",
    "yy = fh.variables['y'][:].copy() #y-coord\n",
    "s_raw = fh.variables['surface'][:].copy() #surface elevation\n",
    "h_raw=fh.variables['thickness'][:].copy() # Gridded thickness\n",
    "b_raw = fh.variables['bed'][:].copy() # bed topo\n",
    "thick_mask = fh.variables['mask'][:].copy()\n",
    "#mask values: 0=ocean, 1=ice-free land, 2=grounded ice, 3=floating ice, 4=non-Greenland land\n",
    "ss = np.ma.masked_where(thick_mask !=2, s_raw) #select only grounded ice cells\n",
    "hh = np.ma.masked_where(thick_mask !=2, h_raw) \n",
    "bb = np.ma.masked_where(thick_mask !=2, b_raw)\n",
    "## Down-sampling\n",
    "X = xx[::2]\n",
    "Y = yy[::2]\n",
    "S = ss[::2, ::2]\n",
    "H = hh[::2, ::2] \n",
    "B = bb[::2, ::2]\n",
    "M = thick_mask[::2,::2]\n",
    "fh.close()\n",
    "\n",
    "#Smoothing bed\n",
    "unsmoothB = B\n",
    "smoothB = gaussian_filter(B, 2)\n",
    "smoothS = gaussian_filter(S, 2) #17 Jan 19 - smoothing S as well for consistency with auto-selected networks\n",
    "#B_processed = np.ma.masked_where(thick_mask !=2, smoothB)\n",
    "\n",
    "S_interp = interpolate.RectBivariateSpline(X, Y[::-1], smoothS.T[::, ::-1])\n",
    "H_interp = interpolate.RectBivariateSpline(X, Y[::-1], H.T[::, ::-1])\n",
    "B_interp = interpolate.RectBivariateSpline(X, Y[::-1], smoothB.T[::, ::-1])\n",
    "def NearestMaskVal(x0,y0):\n",
    "    x_idx = np.abs(X-x0).argmin()\n",
    "    y_idx = np.abs(Y-y0).argmin()\n",
    "    return M[y_idx, x_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reading in surface mass balance from 1981-2010 climatology')\n",
    "gl_smb_path ='/Users/lizz/Documents/GitHub/Data_unsynced/HIRHAM5-SMB/DMI-HIRHAM5_GL2_ERAI_1980_2014_SMB_YM.nc'\n",
    "fh2 = Dataset(gl_smb_path, mode='r')\n",
    "x_lon = fh2.variables['lon'][:].copy() #x-coord (latlon)\n",
    "y_lat = fh2.variables['lat'][:].copy() #y-coord (latlon)\n",
    "#zs = fh2.variables['height'][:].copy() #height in m - is this surface elevation or SMB?\n",
    "ts = fh2.variables['time'][:].copy()\n",
    "smb_raw = fh2.variables['smb'][:].copy()\n",
    "fh2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bed topography from BedMachine is on a Polar Stereographic North grid (EPSG code 3413).  The surface mass balance from HIRHAM is on a lat-lon grid with WGS 84 datum (EPSG code 4326).  We use `pyproj` to align the grids, then interpolate the yearly mass balance so that we can extract at each glacier site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Now transforming coordinate system of SMB')\n",
    "wgs84 = pyproj.Proj(\"+init=EPSG:4326\") # LatLon with WGS84 datum used by GPS units and Google Earth\n",
    "psn_gl = pyproj.Proj(\"+init=epsg:3413\") # Polar Stereographic North used by BedMachine (as stated in NetCDF header)\n",
    "xs, ys = pyproj.transform(wgs84, psn_gl, x_lon, y_lat)\n",
    "Xmat, Ymat = np.meshgrid(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hindcasting SMB: year-specific 2006-2014\n",
    "SMB_dict = {} #set up a dictionary of surface mass balance fields indexed by year\n",
    "for year in range(2006, 2015):\n",
    "    index = year - 2015 #so that 2014 will be smb_raw[-1], etc.\n",
    "    smb_year = smb_raw[index][0]\n",
    "    regridded_smb_year = interpolate.griddata((xs.ravel(), ys.ravel()), smb_year.ravel(), (Xmat, Ymat), method='nearest')\n",
    "    SMB_dict[year] = interpolate.interp2d(X, Y, regridded_smb_year, kind='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in saved glaciers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify flowline coordinates and optimise the yield strength for each glacier in a separate pre-processing routine.  We load in the saved results, tagged by their MEaSUREs glacier IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reading in optimal yield strength dictionary')\n",
    "optimal_taus_fpath = '/Users/lizz/Documents/GitHub/Data_unsynced/Auto_selected-networks/Optimization_analysis/bestfit_taus-B_S_smoothing-fromdate_2019-01-17.csv'\n",
    "f_ot = open(optimal_taus_fpath, 'r')\n",
    "header = f_ot.readline()\n",
    "hdr = header.strip('\\r\\n')\n",
    "optimal_taus = {}\n",
    "lines = f_ot.readlines()\n",
    "for i, l in enumerate(lines):\n",
    "    linstrip = l.strip('\\r\\n')\n",
    "    parts = linstrip.split(',')\n",
    "    gid = int(parts[0]) #MEaSUREs glacier ID is first\n",
    "    tau_y = float(parts[1]) #yield strength in Pa\n",
    "    yieldtype = parts[2] #'constant' or 'variable' string\n",
    "    optimal_taus[gid] = (tau_y, yieldtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify the base filepath where glacier networks are stored, and which glacier IDs should be available to simulate.  Choose which glacier(s) to simulate by changing `gids_totest` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_fpath = '/Users/lizz/Documents/GitHub/Data_unsynced/Auto_selected-networks/Gld-autonetwork-GID'\n",
    "\n",
    "glacier_ids = range(1,195) #range of MEaSUREs IDs\n",
    "not_present = (93, 94, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 169) #glacier IDs missing from set\n",
    "added_jan19 = (139, 140, 141, 142, 143, 159, 161, 172, 173, 177)\n",
    "for n in not_present:\n",
    "    try:\n",
    "        glacier_ids.remove(n)\n",
    "    except ValueError:\n",
    "        pass\n",
    "gids_totest = (3,) #test specific glaciers; set ==glacier_ids to test all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating glacier networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the settings for the simulation.  If you had correct input data, you could change `testyears` to simulate a longer period (or change the last argument to simulate denser/coarser in time).  You can also switch debug mode on or off by changing `db`, change Glen's A parameter by adjusting `test_A`, and, if applicable, change the climate scenario by changing `persistence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulation settings\n",
    "testyears = arange(0, 9, 0.25) #test only 2006-2015, for comparison\n",
    "start_year=2006 #determined by which MEaSUREs termini we used to initialize a given set\n",
    "db = True #debug mode, True for frequent print statements\n",
    "test_A, icetemp = 3.5E-25, 'min10C' # -10 C, good guess for Greenland\n",
    "scenario = 'persistence'\n",
    "output_heavy = False #pref for output--should be False for efficient running, True for saving full surface profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the simulation for each glacier selected and save the output as a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_gids = []\n",
    "\n",
    "for gid in gids_totest:\n",
    "    print 'Reading in glacier ID: '+str(gid)\n",
    "    if gid in added_jan19:\n",
    "        filename = base_fpath+str(gid)+'-date_2019-01-10.csv'\n",
    "    elif gid<160:\n",
    "        filename = base_fpath+str(gid)+'-date_2018-10-03.csv'\n",
    "    else:\n",
    "        filename = base_fpath+str(gid)+'-date_2018-10-04.csv' #workaround because I ran these in batches and saved them with the date\n",
    "    \n",
    "    coords_list = Flowline_CSV(filename, has_width=True, flip_order=False)\n",
    "    nlines = len(coords_list)\n",
    "    branch_0 = Branch(coords=coords_list[0], index=0, order=0) #saving central branch as main\n",
    "    branch_list = [branch_0]\n",
    "    if nlines>0:\n",
    "        for l in range(1, nlines):\n",
    "            branch_l = Branch(coords = coords_list[l], index=l, order=1, flows_to=0)\n",
    "            branch_list.append(branch_l)\n",
    "    nw = PlasticNetwork(name='GID'+str(gid), init_type='Branch', branches=branch_list, main_terminus=branch_0.coords[0])\n",
    "    nw.make_full_lines()\n",
    "\n",
    "    print 'Now processing glacier ID: '+str(gid)\n",
    "    nw.process_full_lines(B_interp, S_interp, H_interp)\n",
    "    nw.remove_floating()\n",
    "    nw.make_full_lines()\n",
    "    nw.process_full_lines(B_interp, S_interp, H_interp)\n",
    "    nw.network_tau = optimal_taus[gid][0]\n",
    "    nw.network_yield_type = optimal_taus[gid][1]\n",
    "    nw.network_ref_profiles()\n",
    "    \n",
    "    ## Simulations forced by SMB\n",
    "    time_varying_smb = [[0.001*(1000/nw.rho_ice)*SMB_dict[int(floor(yr))](nw.flowlines[0].coords[i,0], nw.flowlines[0].coords[i,1]) for i in range(len(nw.flowlines[0].coords))] for yr in start_year+testyears]\n",
    "    catchment_smb_vals = [np.mean(time_varying_smb[i]) for i in range(len(time_varying_smb))] #because grid spacing is very fine, mean of all points should approximate integral/length\n",
    "    print 'shape(catchment_smb_vals) = {}'.format(shape(catchment_smb_vals))\n",
    "    nw.smb_alphadot = catchment_smb_vals[0] #initial SMB\n",
    "    print 'a_dot from SMB: {}'.format(nw.smb_alphadot)\n",
    "    nw.terminus_adot = time_varying_smb[0][0]\n",
    "    print 'Terminus a_dot: {}'.format(nw.terminus_adot)\n",
    "    try:\n",
    "        nw.terminus_time_evolve(testyears=testyears, alpha_dot_variable=catchment_smb_vals, \n",
    "                                dL=1/L0, separation_buffer=10000/L0, rate_factor=test_A,\n",
    "                                has_smb=True, terminus_balance=nw.terminus_adot, submarine_melt = 0, \n",
    "                                debug_mode=db, output_heavy=output_heavy, sl_contribution=False\n",
    "                                )\n",
    "    except:\n",
    "        bad_gids.append(gid)   \n",
    "        continue \n",
    "    \n",
    "    print 'Saving output for {}'.format(nw.name)\n",
    "    fn = str(nw.name)\n",
    "    fn1 = fn.replace(\" \", \"\")\n",
    "    fn2 = fn1.replace(\"[\", \"-\")\n",
    "    fn3 = fn2.replace(\"/\", \"_\")\n",
    "    fn4 = fn3.replace(\"]\", \"\")\n",
    "    fn5 = '/Users/lizz/Desktop/Hindcasted_networks/'+fn4+'-{}-{}-{}ice-{}a_dt025a.pickle'.format(datetime.date.today(), scenario, icetemp, int(max(testyears)))\n",
    "    nw.save_network(filename=fn5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, report which glaciers failed to run in a csv output of bad glacier IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Output errors to csv file\n",
    "error_fn = '/Users/lizz/Desktop/Hindcasted_networks/error_gids-{}.csv'.format(datetime.date.today())\n",
    "np.savetxt(error_fn, np.asarray(bad_gids), delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
